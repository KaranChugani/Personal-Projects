# Project description

This file will describe the steps taken to train and implement a machine learning pipeline for the classification of human activities using gyroscope and accelerometer data.To train the classifier a dataset which included 24000 observations of 5 different activities (Sitting, Standing, Walking, Jogging and Martial Arts) was used. This is provided in this folder in data.m file. (Note that features had already been extracted)

PCA to explore the data:
The first step taken consisted in performing PCA (Principal Component Analysis), which is a technique that can reduce the dimensionality of a dataset by applying orthogonal linear transformations, which can transform large datasets into a new coordinate system where its features represent the differences amongst the directions with the greatest variances.
The advantage of PCA over using raw directly is that in most cases the number of principal components required to “explain” a high amount of variance in the data is much lower than the dimensionality of the original dataset, which can reduce the computational time it takes to train and test our classifier. Another advantage of PCA is that it allows the visualizing of how the different classes in our dataset are arranged, by plotting the first two or three principal components for each observation. A graphic observation of these components can give an indication of how well the different classes can be distinguished as well as showing outliers that don’t match the clustering patterns of the different classes. The code provided in PlotPCA.m can performs PCA on the datasets and plots the first 3 and first 2 components in different figures. 

Choosing a classifier:
To make the decision as to which classifier to choose a literature review was made, specifically on papers that compared various classification techniques in the context of human activity recognition. A 2014 paper by researchers in the Ostrava Technical University compared the performance of various classifiers on three different Human Activity Recognition (HAR) datasets. Some of these classifiers are: Quadratic Discriminant Analysis (QDA), Random Forests (RF) and k-Nearest Neighbour (k-NN) amongst others. Results showed that k-NN and RF had the best performance [1]. Another paper published by researchers in Oulu University (Finland) and Waseda University (Japan) compared the performance of k-NN algorithm against a multilayer perceptron for the classification of 17 different activities. Results showed that the k-NN algorithm slightly outperformed the multilayer perceptron (92.89% against 89.76%) [2] . Due to its simplicity in design compared to other classifiers and its high classification accuracy, it was decided to implement the k-NN algorithm.

How does the K-Nearest Neighbours algorithms work:
The k-NN algorithm works in a very simple way. First, a set of training data is mapped onto a feature space, without making any changes to the structure of the data. Then, to classify a new data point, a loop that finds the observations that are closest to the new testing observation (nearest neighbours) are found. This is where k comes in. If k = 1, the class assigned to the new observation is equal to the class of its nearest neighbour. If k > 1, a majority vote is taken amongst the k nearest to decide the class of the new observation. When there is a tie in the majority vote, the value of K is reduced until there was no longer a tie. The two main parameters that are used to tune the k-NN are the number k and the method used to calculate distance between different points.

Data preprocessing:
First, Principal component analysis was applied to the dataset, to reduce its dimensionality for faster training and testing of the data. As it was vital to not lose any important information from the data, the number of principal components that explain 99.99% of the variance were calculated. This number was found to be 11, so all tests were performed using the scores of the 11 first principal components as features for each of the observations.

Cross-validation:
To ensure a fair test N-fold Cross validation was used. This is a technique that allows for the dataset to be split into N subsets of which N-1 will be used for training and 1 will be used for testing. Each round of the algorithm involves using one of these subsets for training and the other subset for testing. Results are averaged over several rounds, which can give a better estimate of the predictive power of the classifier due to the significant reduction in bias. All the following tests were performed using 5-fold cross validation.

Hyperparameter tuning: 
